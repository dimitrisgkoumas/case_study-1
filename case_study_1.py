# -*- coding: utf-8 -*-
"""case_study#1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QoSDNNtJ-6pmu6X_-Tr3sj4MWPCNvX18
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from google.colab import files
import seaborn as sns

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

# Load files
df = pd.read_csv('/gdrive/My Drive/Colab Notebooks/loans_full_schema.csv')

df.info()
df.head()

df.columns.to_list()

for col in df.columns:
  print(col,df[col].value_counts())

df['issue_month']=pd.to_datetime(df['issue_month']).dt.month
df['issue_month'].value_counts()

df2=df.copy()# for visualization cat
cat=[col for col in df.columns if df[col].dtypes=='object']
print(cat)
for col in cat:
  df[col]=df[col].astype('category').cat.codes
df.info()

for col in cat:
  print(df[col].value_counts())

for col in df.columns:
    print(col,df[col].isnull().sum())

df.duplicated().any()

df.fillna(0,inplace=True)
df.info()
df.head()

#visualization
sns.histplot(data=df, x="interest_rate", kde=True)

plt.figure(figsize=(40,30))
sns.heatmap(df.drop(cat,axis=1).corr(),annot=True)

cat

plt.figure(figsize=(30,20))

b=sns.boxplot(y="loan_purpose", x="interest_rate", data=df2,orient="h")
b.tick_params(labelsize=25)
del df2

sns.set_style("darkgrid")

plt.figure(figsize=(30,20))
# use the scatterplot function
sns.scatterplot(data=df, x="loan_amount", y="interest_rate",hue='term',palette="viridis", edgecolors="black", alpha=1)
#

#plt.figure(figsize=(30,20))
g=sns.JointGrid(y="debt_to_income" , x="interest_rate", data=df, height = 8,hue='term')
g = g.plot_joint(sns.kdeplot , cmap="OrRd_r")
g = g.plot_marginals(sns.kdeplot, shade=True , color = 'red')

#--Model

# Select main columns to be used in training
main_cols = df.columns.difference(['interest_rate'])

X = df[main_cols]
y = df['interest_rate']

seed=45
from yellowbrick.regressor import ResidualsPlot
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.model_selection import GridSearchCV,StratifiedKFold,RepeatedKFold
from sklearn.model_selection import KFold
from sklearn.ensemble import VotingRegressor
import xgboost as xgb
    
         
        


            
       

        
kf = KFold(n_splits=10,shuffle=True,random_state=seed)
    
    

i=0
for train_index,test_index in kf.split(X,y):

        i+=1
        
        xtr,xvl = X.loc[train_index],X.loc[test_index]
        ytr,yvl = y.loc[train_index],y.loc[test_index]      


         
        alg1=lgb.LGBMRegressor()
        alg2 = xgb.XGBRegressor(objective ='reg:squarederror')
        ereg = VotingRegressor(estimators=[('lgb',alg1), ('xgb',alg2)])

        visualizer = ResidualsPlot(ereg)

        visualizer.fit(xtr, ytr)  # Fit the training data to the visualizer
        visualizer.score(xvl, yvl)  # Evaluate the model on the test data
        print('Fold'+str(i))
        visualizer.show()

from sklearn.metrics import r2_score
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)
preds=ereg.fit(X_train,y_train).predict(X_test)
r2 = r2_score(preds,y_test)
print(r2)

